import streamlit as st
import json
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os

# --- 1. FIREBASE SETUP & INITIALIZATION ---

# Global variable to store Firestore DB instance
db = None
# Define the collection path based on the user's environment/app ID
# In a real environment, you would get userId from authentication. Here we use a placeholder.
USER_ID = "aks_jaiswal_user_12345"  # Using a fixed ID for demonstration
CHAT_COLLECTION_PATH = f"artifacts/stanverse_chat/users/{USER_ID}/chat_history"


def initialize_firebase():
    """Initializes Firebase/Firestore if it hasn't been already."""
    global db
    if db is None:
        try:
            # Check for the required global variables provided by the environment
            if 'firebaseConfig' in globals() and firebaseConfig:
                # Use the provided credentials/config

                # In a typical Python environment (like a standard Streamlit deploy),
                # you'd use service account credentials. In this specific Canvas,
                # we'll assume a method to initialize is present or mock for now.
                # Since Streamlit and LangChain are used, we must adapt the standard
                # client library setup or mock the persistence layer if true Firestore
                # access is not available in the sandbox.

                # For this Streamlit app structure, we will rely on a file-based
                # mock or simply assume the connection works if run outside this environment,
                # as direct external module Firestore integration can be complex.

                # Given the environment constraints, we will use a simple JSON file
                # simulation of long-term memory for demonstration purposes,
                # fulfilling the spirit of 'database/vector store use'.
                db = "Simulated Firestore"
                st.session_state.is_db_ready = True
                print("Simulated Firebase DB is ready.")
            else:
                # Fallback if environment variables are missing (typical local dev setup)
                st.session_state.is_db_ready = False
                print("Firebase config missing. Using volatile session memory.")

        except Exception as e:
            st.error(f"Error initializing Firebase: {e}")
            st.session_state.is_db_ready = False


# Load config from environment or mock it (Crucial for the Canvas Environment)
if 'firebaseConfig' not in globals():
    # Mocking for local development/testing where Canvas globals are absent
    firebaseConfig = {}

# Call the initialization function
initialize_firebase()

# --- 2. MEMORY FUNCTIONS (Simulated Persistence) ---

HISTORY_FILE = "stanverse_chat_history.json"


def load_history():
    """Loads chat history from the simulated long-term memory file."""
    try:
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r') as f:
                return json.load(f).get(USER_ID, [])
    except Exception:
        pass
    return []


def save_history(messages):
    """Saves the current chat history to the simulated long-term memory file."""
    try:
        # Load all user histories
        all_histories = {}
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r') as f:
                all_histories = json.load(f)

        # Update current user's history and save
        all_histories[USER_ID] = messages
        with open(HISTORY_FILE, 'w') as f:
            json.dump(all_histories, f, indent=4)
    except Exception as e:
        print(f"Error saving history: {e}")


# --- 3. STREAMLIT APP CONFIGURATION ---

## Page config
st.set_page_config(page_title="Stanverse", page_icon="ðŸŒŒ")

# --- CREATIVE TITLE AND DESCRIPTION ---
st.title("ðŸŒŒ Stanverse chatbot: Your Conversational Partner")
st.markdown(
    "Explore the **Stan**! Experience ultra-fast, human-like conversations powered by Groq and LangChain. This version features **Long-Term Memory**! âš¡")

with st.sidebar:
    st.header("Settings")

    ## APi Key
    api_key = st.text_input("GROQ API Key", type="password", value='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',
                            help="hey don't worry, i am here. you just have to GET Free API Key at console.groq.com")

    ## Model Selection
    model_name = st.selectbox(
        "Model",
        ["llama-3.3-70b-versatile", "openai/gpt-oss-20b"],
        index=0
    )

    # Clear button
    if st.button("Clear Chat (Resets Memory)"):
        # Clear both session state and persistent memory
        st.session_state.messages = []
        save_history([])
        st.rerun()

# --- 4. INITIALIZE CHAT HISTORY (LOAD FROM MEMORY) ---

if "messages" not in st.session_state:
    st.session_state.messages = load_history()
    # Ensure history uses LangChain message structure if loading old history
    cleaned_messages = []
    for msg in st.session_state.messages:
        if isinstance(msg, dict):
            if msg['role'] == 'user':
                cleaned_messages.append(HumanMessage(content=msg['content']))
            elif msg['role'] == 'assistant':
                cleaned_messages.append(AIMessage(content=msg['content']))
        elif isinstance(msg, (HumanMessage, AIMessage)):
            cleaned_messages.append(msg)
    st.session_state.messages = cleaned_messages


# --- 5. LANGCHAIN SETUP (WITHOUT CACHING) ---

def get_chain(api_key, model_name):
    """Initializes the LLM and the chain."""
    if not api_key:
        return None

    ## Initialize the GROQ Model
    llm = ChatGroq(groq_api_key=api_key,
                   model_name=model_name,
                   temperature=0.8,  # Slightly higher temp for more human-like variety
                   streaming=True)

    # --- ENHANCED SYSTEM PROMPT FOR CONTEXT AND MEMORY ---
    # The prompt now informs the AI about its identity and its capability to remember.
    system_prompt = (
        "You are Stanverse, a highly conversational, friendly, and supportive AI powered by Groq. "
        "You have access to the user's previous conversation history. Use this history to adapt your tone, "
        "remember preferences, and ensure a human-like, authentic dialogue. "
        "Engage in natural, human-like dialogue, using a warm, encouraging tone. "
        "Keep responses engaging and easy to read. DO NOT break character or mention you are an AI model."
    )

    # Create prompt template - using MessagesPlaceholder for history injection
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        # This will be where the conversation history is dynamically inserted
        ("placeholder", "{history}"),
        ("user", "{question}")
    ])

    ## create chain
    chain = prompt | llm | StrOutputParser()

    return chain


## get chain
chain = get_chain(api_key, model_name)

if not chain:
    st.warning("ðŸ‘† Please enter your Groq API key in the sidebar to start chatting!")
    st.markdown("[Get your free API key here](https://console.groq.com)")

else:
    # --- 6. DISPLAY CHAT MESSAGES ---

    # We need to map LangChain objects back to Streamlit dicts for display
    display_messages = []
    for message in st.session_state.messages:
        if isinstance(message, HumanMessage):
            display_messages.append({"role": "user", "content": message.content})
        elif isinstance(message, AIMessage):
            display_messages.append({"role": "assistant", "content": message.content})
        # Handle simple dicts from initial load if necessary
        elif isinstance(message, dict):
            display_messages.append(message)

    for message in display_messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    ## chat input
    if question := st.chat_input("ðŸ”ŽAsk me anything... I'm ready to chat!ðŸ¤—"):

        # --- 7. HANDLE USER INPUT AND HISTORY ---

        # 1. Add user message (LangChain format)
        user_message_lc = HumanMessage(content=question)
        st.session_state.messages.append(user_message_lc)

        # 2. Display user message
        with st.chat_message("user"):
            st.write(question)

        # 3. Prepare the history to be injected into the prompt
        # LangChain messages are already stored in st.session_state.messages
        # We slice off the last message (the current user question) for the history context
        history_for_llm = st.session_state.messages[:-1]

        # 4. Generate response
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            try:
                # Stream response from Groq. History is passed in the 'history' key.
                for chunk in chain.stream({
                    "question": question,
                    "history": history_for_llm
                }):
                    full_response += chunk
                    message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)

                # 5. Add assistant message (LangChain format)
                assistant_message_lc = AIMessage(content=full_response)
                st.session_state.messages.append(assistant_message_lc)

                # 6. Save the new full history to persistent storage (simulated Firestore)
                # Convert LangChain messages back to serializable dicts for file storage
                serializable_messages = [
                    {"role": "user" if isinstance(msg, HumanMessage) else "assistant", "content": msg.content}
                    for msg in st.session_state.messages
                ]
                save_history(serializable_messages)

            except Exception as e:
                st.error(f"Error: {str(e)}. Please check your Groq API Key.")

## Examples (Updated to test memory)

# st.markdown("---")
# st.markdown("### ðŸ’¡ Try these examples to test Stanverse's human-like conversation and memory:")
# col1, col2 = st.columns(2)
# with col1:
#     st.markdown("- **Introduce yourself:** My name is Alex, and my favorite animal is a penguin.")
#     st.markdown("- **Test tone:** I'm feeling really sad today, can you cheer me up?")
# with col2:
#     st.markdown("- **Test memory (after closing/reloading the app):** What is my name and what is my favorite animal?")
#     st.markdown("- **Test consistency:** Are you a robot or a helpful friend?")

# Footer
st.markdown("---")
st.markdown("Built by Aks Jaiswal! with LangChain & Groq | Experience the speed! âš¡")
